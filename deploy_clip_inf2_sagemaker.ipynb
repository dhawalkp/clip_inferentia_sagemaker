{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a91ad06",
   "metadata": {},
   "source": [
    "# Deploy Clip text models on a Inferentia2 Custom chip with SageMaker and LMI Containers with Neuron compilers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a96d30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|lab2-stable-diffusion|option1-aitemplate|lmi-aitemplate-stablediff.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd333a1",
   "metadata": {},
   "source": [
    "**In this notebook we will host Clip  SageMaker using LMI containers**\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the [Large Model Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) container that is optimized for hosting large models using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent [blog post](https://aws.amazon.com/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/).\n",
    "\n",
    "The world of artificial intelligence (AI) and machine learning (ML) has been witnessing a paradigm shift with the rise of generative AI models that can create human-like text, images, code, and audio. Compared to classical ML models, generative AI models are significantly bigger and more complex. However, their increasing complexity also comes with high costs for inference and a growing need for powerful compute resources. The high cost of inference for generative AI models can be a barrier to entry for businesses and researchers with limited resources, necessitating the need for more efficient and cost-effective solutions. Furthermore, the majority of generative AI use cases involve human interaction or real-world scenarios, necessitating hardware that can deliver low-latency performance. AWS has been innovating with purpose-built chips to address the growing need for powerful, efficient, and cost-effective compute hardware.\n",
    "\n",
    "\n",
    "This notebook was tested on a `inf2.8xlarge` instance \n",
    "\n",
    "\n",
    "Model license: By using this model, please review and agree to the https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/LICENSE-MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6d927",
   "metadata": {},
   "source": [
    "### Overview of Inferentia and trn1 instances\n",
    "\n",
    "Overview of ml.trn1 and ml.inf2 instances\n",
    "ml.trn1 instances are powered by the Trainium accelerator, which is purpose built mainly for high-performance deep learning training of generative AI models, including LLMs. However, these instances also support inference workloads for models that are even larger than what fits into Inf2. The largest instance size, trn1.32xlarge instances, features 16 Trainium accelerators with 512 GB of accelerator memory in a single instance delivering up to 3.4 petaflops of FP16/BF16 compute power. 16 Trainium accelerators are connected with ultra-high-speed NeuronLinkv2 for streamlined collective communications.\n",
    "\n",
    "ml.Inf2 instances are powered by the AWS Inferentia2 accelerator, a purpose built accelerator for inference. It delivers three times higher compute performance, up to four times higher throughput, and up to 10 times lower latency compared to first-generation AWS Inferentia. The largest instance size, Inf2.48xlarge, features 12 AWS Inferentia2 accelerators with 384 GB of accelerator memory in a single instance for a combined compute power of 2.3 petaflops for BF16/FP16. It enables you to deploy up to a 175-billion-parameter model in a single instance. Inf2 is the only inference-optimized instance to offer this interconnect, a feature that is only available in more expensive training instances. For ultra-large models that donâ€™t fit into a single accelerator, data flows directly between accelerators with NeuronLink, bypassing the CPU completely. With NeuronLink, Inf2 supports faster distributed inference and improves throughput and latency.\n",
    "\n",
    "Both AWS Inferentia2 and Trainium accelerators have two NeuronCores-v2, 32 GB HBM memory stacks, and dedicated collective-compute engines, which automatically optimize runtime by overlapping computation and communication when doing multi-accelerator inference. For more details on the architecture, refer to Trainium and Inferentia devices.\n",
    "\n",
    "For more details refer to [Neuron Docs](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/model-architecture-fit.html#diffusion-models)\n",
    "\n",
    "<img src=\"images/Neuron-arch.jpeg\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f58b6",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model for Deployment\n",
    "As a first step, we'll import the relevant libraries and configure several global variables such as the hosting image that will be used nd the S3 location of our model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d4296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f48b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jinja2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bb236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# variables\n",
    "s3_client = None  # boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e894307",
   "metadata": {},
   "source": [
    "### If you are running this notebook from outside of AWS \n",
    "Please configure the Appropriate access keys and the Role you would like to assume and ensure the access to that role is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3615426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<your_values>\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<your_values>\"\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<your_values>\" #\"us-east-1\"\n",
    "\n",
    "# os.environ['ASSUME_ROLE'] = \"<your_values>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a1c28",
   "metadata": {},
   "source": [
    "### Please upload the model weights (i.e. resblocks_traced.pt) available in model dir of this repo in your S3 bucket and replace the value of s3_model_prefix with the s3 URI consisting of model weights in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d87e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = None\n",
    "bucket = None\n",
    "region = None\n",
    "account_id = None\n",
    "\n",
    "try:\n",
    "\n",
    "    role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "    sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "    bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "    region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "    account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "except:\n",
    "    print(\"error in sagemaker roles via SDK. Please Configure appropriate values below\")\n",
    "    # - Define the buckets to be used\n",
    "\n",
    "# - Define the s3 prefixes\n",
    "\n",
    "s3_code_prefix = (\n",
    "    \"clip/neuron/code_sd_g5\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "s3_model_prefix = f\"s3://<>\"\n",
    "print(f\"public s3:url --- > {s3_model_prefix}::\")\n",
    "\n",
    "\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates\n",
    "\n",
    "print(f\"Current location to be used for Weights={s3_model_prefix}\")\n",
    "print(f\"Current location to be used for Code:prefix={s3_code_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcd970",
   "metadata": {},
   "source": [
    "#### Use boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assumed_role = os.getenv(\"ASSUME_ROLE\", None)\n",
    "print(assumed_role)\n",
    "boto3_kwargs = {}\n",
    "session = boto3.Session()\n",
    "if assumed_role:\n",
    "    sts = session.client(\"sts\")\n",
    "    response = sts.assume_role(RoleArn=str(assumed_role), RoleSessionName=\"temp-inf2-llm-1\")\n",
    "    print(f\"ROLE:assumed!!!{response}\")\n",
    "    boto3_kwargs = dict(\n",
    "        aws_access_key_id=response[\"Credentials\"][\"AccessKeyId\"],\n",
    "        aws_secret_access_key=response[\"Credentials\"][\"SecretAccessKey\"],\n",
    "        aws_session_token=response[\"Credentials\"][\"SessionToken\"],\n",
    "    )\n",
    "    print(\"Making the role be the Asumed role !!!\")\n",
    "    role = assumed_role\n",
    "\n",
    "boto3_sm_run_client = boto3.client(\"sagemaker-runtime\", region_name=region, **boto3_kwargs)\n",
    "# print(boto3_sm_client)\n",
    "\n",
    "boto3_sm_client = boto3.client(\"sagemaker\", region_name=region, **boto3_kwargs)\n",
    "\n",
    "boto3_s3_client = boto3.client(\"s3\", region_name=region, **boto3_kwargs)\n",
    "\n",
    "boto3_iam_client = boto3.client(\"iam\", region_name=region, **boto3_kwargs)\n",
    "\n",
    "print(f\"Role:assumed={role}\")\n",
    "boto3_s3_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - test boto3 works\n",
    "boto3_s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795552f",
   "metadata": {},
   "source": [
    "### Part 2 - Create the model.tar.gz\n",
    "\n",
    "This file is the custom inference script for loading and then calling the models. The model weights have been compiled for specific Hardware as explained below. For convinience the compiled weights will be out in a public S3 location for easy reference. However it is important to note that DJL comes with a pre-built handlers which can be found here [Default handlers](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py)\n",
    "\n",
    "**Model weights**\n",
    "\n",
    "This notebook leverages the open-clip-torch  2.20.0  for HuggingFace Stable Diffusion 2.1 (512x512) model for accelerated inference on Neuron. For Stable Diffusion 768x768, please see the notebook named [Opn Clip](https://github.com/mlfoundations/open_clip)\n",
    "\n",
    "\n",
    "**Some important points for compiling the model**\n",
    "\n",
    "run them in AOt and save them on S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e609b5",
   "metadata": {},
   "source": [
    "**Environment for compilations**\n",
    "\n",
    "Following the Inf2 set up you will find a VENV pre created with the following pip packages installed:\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `diffusers==0.14.0`\n",
    "- `transformers==4.26.1`\n",
    "- `accelerate==0.16.0`\n",
    "- `open-clip-torch==2.20.0`\n",
    "\n",
    "`torch-neuronx` and `neuronx-cc` will be installed when you configure your environment following the Inf2 setup guide. The remaining dependencies can be installed as:\n",
    "\n",
    "```\n",
    "diffusers==0.14.0\n",
    "transformers==4.26.1\n",
    "accelerate==0.16.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90538542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf code_sd\n",
    "!mkdir -p code_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ad10a",
   "metadata": {},
   "source": [
    "There are a few options specified here. Lets go through them in turn<br>\n",
    "1. `engine` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the [DJL Python Engine](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python)\n",
    "2. `option.entryPoint` - specifies the entrypoint code that will be used to host the model. djl_python.huggingface refers to the `huggingface.py` module from [djl_python repo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python).  \n",
    "3. `option.s3url` - specifies the location of the model files. Alternativelly an `option.model_id` option can be used instead to specifiy a model from Hugging Face Hub (e.g. `EleutherAI/gpt-j-6B`) and the model will be automatically downloaded from the Hub. The s3url approach is recommended as it allows you to host the model artifact within your own environment and enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance \n",
    "4. `option.task` - This is specific to the `huggingface.py` inference handler and specifies for which task this model will be used\n",
    "5. `option.tensor_parallel_degree` - Enables layer-wise model partitioning through .\n",
    "6. `option.load_in_8bit` - Quantizes the model weights to int8 thereby greatly reducing the memory footprint of the model from the initial FP32. See this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) from Hugging Face for additional information \n",
    "\n",
    "For more information on the available options, please refer to the [SageMaker Large Model Inference Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_sd/serving.properties\n",
    "engine = Python\n",
    "#option.model_id=/opt/ml/model/clip_model\n",
    "option.model_id = {{s3url}}\n",
    "# option.entryPoint=djl_python.transformers-neuronx\n",
    "option.tensor_parallel_degree = 2\n",
    "option.model_loading_timeout=2400\n",
    "option.enable_streaming=False\n",
    "# option.dtype=bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_model_prefix='s3://sagemaker-us-east-2-414210492846/clip/inf2/model_clip/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b42ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"code_sd/serving.properties\").open().read())\n",
    "Path(\"code_sd/serving.properties\").open(\"w\").write(template.render(s3url=s3_model_prefix))\n",
    "!pygmentize code_sd/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_sd/requirements.txt\n",
    "open_clip_torch==2.20.0\n",
    "--extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    "transformers-neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a12885",
   "metadata": {},
   "source": [
    "#### Bring your own inference script \n",
    "compiled_model/text_encoder.pt does not exist\n",
    "We will load and replace the following in the base model\n",
    "In particular, we will load the compiled weights for :\n",
    "1. The CLIP text encoder;\n",
    "2. The VAE decoder;\n",
    "3. The UNet, and\n",
    "4. The VAE_post_quant_conv\n",
    "\n",
    "These blocks are chosen because they represent the bulk of the compute in the pipeline. Further we will also load the Replace original cross-attention module with custom cross-attention module for better performance\n",
    "    CrossAttention.get_attention_scores = get_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57286c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code_sd/model.py\n",
    "import traceback\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch_neuronx\n",
    "import open_clip\n",
    "import os\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - define some clases\n",
    "def partial_encode(model,text, normalize=False):\n",
    "    cast_dtype = model.transformer.get_cast_dtype()\n",
    "\n",
    "    x = model.token_embedding(text).to(cast_dtype)\n",
    "    \n",
    "    x = x + model.positional_embedding.to(cast_dtype)\n",
    "    x = x.permute(1,0,2)\n",
    "    return x,model.attn_mask\n",
    "\n",
    "class CustomTransformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,transformer,traced_resblocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = transformer\n",
    "        self.resblocks = traced_resblocks\n",
    "\n",
    "    def get_cast_dtype(self):\n",
    "        return self.transformer.resblocks[0].mlp.c_fc.weight.dtype\n",
    "\n",
    "    def forward(self,x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n",
    "        return self.resblocks(x,attn_mask)\n",
    "\n",
    "class CustomResAttns(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,resblocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resblocks = resblocks\n",
    "    \n",
    "    def forward(self,x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n",
    "        for r in self.resblocks:\n",
    "            x = r(x,attn_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_and_compile_model(path_save=\"./model/resblocks_traced.pt\"):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "    logging.info(f\"load_and_compile_model(): model:loaded:pretrained\")\n",
    "    \n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
    "    model.eval()\n",
    "    model.forward = model.encode_text\n",
    "    txt = ['The quick brown fox jumped over the lazy dog']\n",
    "    tokens = tokenizer(txt)\n",
    "\n",
    "    x,attn_mask = partial_encode(model,tokens)\n",
    "\n",
    "    if os.path.exists(path_save):\n",
    "        neuron_resblocks = torch.jit.load(path_save)\n",
    "        logging.info(f\"load_and_compile_model():jit:loaded:model\")\n",
    "    else:\n",
    "        neuron_resblocks = torch_neuronx.trace(CustomResAttns(model.transformer.resblocks),(x,attn_mask))\n",
    "        torch.jit.save(neuron_resblocks, path_save)\n",
    "        logging.info(f\"load_and_compile_model():Finally compiled !! and saved {path_save}:\")\n",
    "\n",
    "    model.transformer = CustomTransformer(model.transformer,neuron_resblocks) \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model(properties):\n",
    "    model_location = properties[\"model_dir\"]\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "        \n",
    "    COMPILED_FILES = f\"{model_location}/resblocks_traced.pt\"\n",
    "    logging.info(f\"Loading model in {COMPILED_FILES}\")\n",
    "\n",
    "    tensor_parallel_degree = int(properties.get(\"tensor_parallel_degree\", 1))\n",
    "    logging.info(f\"Loading model in tensor_parallel_degree= {tensor_parallel_degree}\")\n",
    "\n",
    "    # For saving compiler artifacts\n",
    "\n",
    "    model, tokenizer = load_and_compile_model(COMPILED_FILES)\n",
    "\n",
    "    \n",
    "    logging.info(f\"Loading model: All Loaded Model:success\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, params):\n",
    "\n",
    "    logging.info(f\"run_inference: prompt:single={prompt}::params={params}::\")\n",
    "    tokens = tokenizer(prompt)\n",
    "    outputs = model(tokens)\n",
    "\n",
    "    logging.info(f\"run_inference: outputs generated shape={outputs.shape}!\")\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    try:\n",
    "        global model\n",
    "        global tokenizer\n",
    "        if not model:\n",
    "            model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        if inputs.is_empty():\n",
    "            return None\n",
    "        start_time = time.time()\n",
    "\n",
    "        data = inputs.get_as_json()\n",
    "\n",
    "        input_sentences = data[\"prompt\"]\n",
    "        params = data.get(\"parameters\", None)\n",
    "\n",
    "        result = run_inference(model, tokenizer, input_sentences, params)\n",
    "        \n",
    "        buff = io.BytesIO()\n",
    "        torch.save(result, buff)\n",
    "        byte_img = buff.getvalue()\n",
    "        \n",
    "        #outputs = Output().add_as_json(result)\n",
    "        outputs = Output().add(byte_img)\n",
    "\n",
    "        logging.info(f\"handle: :TIME:TAKEN:{ (time.time() - start_time) * 1000}:ms:::\")\n",
    "    except:\n",
    "        excep_str = traceback.format_exc()\n",
    "        logging.info(f\"error:in handle():: traceback={excep_str}:\")\n",
    "        outputs = Output().error(excep_str)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac27055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`\n",
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660bded",
   "metadata": {},
   "source": [
    "### Upload the Tar file to S3 for Creation of End points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45204838",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", region_name=region, **boto3_kwargs)\n",
    "if not s3_resource.Bucket(bucket) in s3_resource.buckets.all():\n",
    "    buck_region = boto3_s3_client.create_bucket(\n",
    "        Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )\n",
    "    print(f\"Bucket created ={buck_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f357c0",
   "metadata": {},
   "source": [
    "## To create the end point the steps are:\n",
    "\n",
    "1. Create the TAR ball with just the serving and the model.py files and upload to S3\n",
    "2. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "3. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.inf2.xlarge\n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 240 to ensure health check starts after the model is ready    \n",
    "\n",
    "    c) VolumeInGB set to a larger value so it can be used for loading the model weights which are 32 GB in size\n",
    "    \n",
    "4. Create the end point using the endpoint config create\n",
    "\n",
    "#### Create the Model\n",
    "Use the image URI for the DJL container and the s3 location to which the tarball was uploaded.\n",
    "\n",
    "The container downloads the model into the `/tmp` space on the container because SageMaker maps the `/tmp` to the Amazon Elastic Block Store (Amazon EBS) volume that is mounted when we specify the endpoint creation parameter VolumeSizeInGB. It leverages `s5cmd`(https://github.com/peak/s5cmd) which offers a very fast download speed and hence extremely useful when downloading large models.\n",
    "\n",
    "For instances like p4dn, which come pre-built with the volume instance, we can continue to leverage the `/tmp` on the container. The size of this mount is large enough to hold the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1700c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - upload_file(Filename, Bucket, Key, ExtraArgs=None, Callback=None, Config=None)\n",
    "\n",
    "boto3_s3_client.upload_file(\n",
    "    \"./model.tar.gz\", bucket, f\"{s3_code_prefix}/model.tar.gz\"\n",
    ")  # - \"path/to/key.txt\"\n",
    "\n",
    "s3_code_artifact = f\"s3://{bucket}/{s3_code_prefix}/model.tar.gz\"\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "boto3_s3_client.list_objects(Bucket=bucket, Prefix=f\"{s3_code_prefix}/model.tar.gz\").get(\n",
    "    \"Contents\", []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88512103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"S3 Model Prefix where the model files are -- > {s3_model_prefix}\")\n",
    "print(f\"S3 Model Bucket is -- > {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522e412",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "Available framework are:\n",
    "- djl-deepspeed (0.20.0, 0.21.0, 0.22.1, 0.23.0)\n",
    "- djl-fastertransformer (0.21.0, 0.22.1, 0.23.0)\n",
    "- fastertransformer (5.3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a1f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/djl-ds:latest\"\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\"djl-neuronx\", region=region, version=\"0.23.0\")\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30863d2",
   "metadata": {},
   "source": [
    "### Creating end point in SageMaker\n",
    "<li>First create a Model\n",
    "<li>Second Create the endpoint config\n",
    "<li>Third Create the endpoint to host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1ed05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b9465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"inf2-clip\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = boto3_sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af47f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = boto3_sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.inf2.xlarge\",  # -\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 360,  # 2400,\n",
    "            \"VolumeSizeInGB\": 400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c87291",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef8f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = boto3_sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca4ff6",
   "metadata": {},
   "source": [
    "### Create the endpoint, and wait for it to be up and running.\n",
    "\n",
    "#### While you wait for the endpoint to be created, you can read more about:\n",
    "- [Clip](https://github.com/mlfoundations/open_clip)\n",
    "- [Deep Learning containers for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)\n",
    "- [DeepSpeed](https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference)   \n",
    "- [Quantization in HuggingFace Accelerate](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
    "- [Handling big models for inference using Accelerate](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map)\n",
    "   \n",
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will generate the image and return that to us using 50 denoising steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1df860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = boto3_sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = boto3_sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34441600",
   "metadata": {},
   "source": [
    "#### Invoke model\n",
    "\n",
    "Let's run an example with a basic text generation prompt `Mountains Landscape`\n",
    "\n",
    "this will create a 512 x 512 resolution picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name='inf2-sd-2023-05-12-16-19-36-982-endpoint'\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dd30b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "response_model = boto3_sm_run_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"prompt\": \"The quick brown fox jumped over the lazy dog\", \"parameters\": {}}),  # \"text_length\": 128}\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2038518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = response_model[\"Body\"].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206c377",
   "metadata": {},
   "source": [
    "#### P95 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f14110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    prompts = [\"Mountains Landscape\"]\n",
    "    response_model = boto3_sm_run_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps({\"prompt\": \"Mountain Landscape\", \"parameters\": {}}),  # \"text_length\": 128}\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4af9f",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "### Part 8 - Clean up <a name=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea0774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "boto3_sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "boto3_sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
